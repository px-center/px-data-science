import os, re, subprocess, time, uuid
import streamlit as st
import vertexai
from vertexai.preview.generative_models import GenerativeModel

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1) ConfiguraÃ§Ã£o inicial
st.set_page_config(page_title="Gemini Chat", page_icon="ğŸ¤–")

os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = ".formal-purpose-354320-af3d391f5234.json"
PROJECT_ID = "formal-purpose-354320"
LOCATION   = os.getenv("GOOGLE_CLOUD_REGION", "us-central1")
MODEL_ID   = "gemini-2.0-flash"

vertexai.init(project=PROJECT_ID, location=LOCATION)
model = GenerativeModel(MODEL_ID)

# Lista para guardar as instÃ¢ncias das IAs
comite_ias = []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2) FunÃ§Ãµes
def limpar_ansi(texto):
    texto = re.sub(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])', '', texto)
    linhas = texto.splitlines()
    linhas_limpa = [linha for linha in linhas if linha.strip()]
    return "\n".join(linhas_limpa)

def estimar_tokens(texto):
    return len(texto) / 4  # AproximaÃ§Ã£o: 1 token â‰ˆ 4 caracteres

def ia_generator(contexto_inicial):
    chat_novo = model.start_chat()
    resposta_instrucao = chat_novo.send_message(contexto_inicial).text
    comite_ias.append(chat_novo)  # Adiciona a IA no comitÃª
    return chat_novo


def ai_analysis(entrada_usuario):
    analise_msg = "ğŸ” **Analisando a entrada pelo comitÃª...**"
    st.chat_message("ğŸ’­ Sistema").markdown(analise_msg)
    messages.append(("system", analise_msg))

    nova_resposta = entrada_usuario  # ComeÃ§a com o prompt

    respostas_comite = []

    for idx, ia in enumerate(comite_ias):
        resposta_ia = ia.send_message(nova_resposta).text

        # Exibe a resposta
        st.chat_message("Gemini").markdown(
            f"**Resposta da IA {idx+1}:**\n\n{resposta_ia}"
        )
        
        messages.append(("bot", f"**Resposta da IA {idx+1}:**\n\n{resposta_ia}"))

        # âœ… NOVO: Verifica se essa resposta tem bloco de cÃ³digo
        if "```python" in resposta_ia:
            executar_codigos(resposta_ia)
            # âš¡âš¡ Continua o loop, nÃ£o interrompe mais!
        
        # Atualiza para a prÃ³xima IA trabalhar sobre a resposta anterior
        nova_resposta = resposta_ia
        respostas_comite.append(resposta_ia)

    # Depois de TODAS as IAs (independente de cÃ³digos encontrados), atualiza tokens
    tokens_saida_follow = estimar_tokens(nova_resposta)
    total_tokens = st.session_state.get("total_tokens", 0) + tokens_saida_follow
    st.session_state["total_tokens"] = total_tokens

    st.markdown(
        f"<div style='color: gray; font-size: small;'>"
        f"Rodada: {int(tokens_saida_follow)} ï½œ Total: {int(total_tokens)}"
        f"</div>",
        unsafe_allow_html=True
    )


def executar_codigos(texto, profundidade=0, max_profundidade=50000):
    if profundidade >= max_profundidade:
        st.warning("ğŸ”„ Limite mÃ¡ximo de ciclos de execuÃ§Ã£o atingido!")
        return

    python_blocks = re.findall(r"```python\s+(.*?)```", texto, re.DOTALL | re.IGNORECASE)
    if not python_blocks:
        return

    codigo = "\n\n".join(python_blocks)
    script_temp = "temp_script.py"
    saida_arquivo = "saida_codigo.txt"

    with open(script_temp, "w", encoding="utf-8") as f:
        f.write(codigo)

    with open(saida_arquivo, "w", encoding="utf-8") as f_saida:
        process = subprocess.Popen(
            ["python", script_temp],
            stdout=f_saida,
            stderr=subprocess.STDOUT,
            text=True
        )

    output_box = st.empty()
    tempo_box = st.empty()
    cancelar_box = st.empty()

    tempo_inicio = time.time()
    cancelado = False

    messages.append(("execucao", "ğŸš€ **Executando cÃ³digo Python...**"))

    while True:
        if cancelar_box.button("âŒ Cancelar ExecuÃ§Ã£o", key=f"cancelar_execucao_{uuid.uuid4()}"):
            if process.poll() is None:
                process.terminate()
                cancelado = True

        if os.path.exists(saida_arquivo):
            with open(saida_arquivo, "r", encoding="utf-8") as f:
                content = f.read()
            content_limpo = limpar_ansi(content)
            output_box.markdown(f"```text\n{content_limpo}\n```")

        tempo_decorrido = time.time() - tempo_inicio
        tempo_box.markdown(f"â±ï¸ **Tempo de execuÃ§Ã£o:** {tempo_decorrido:.1f} segundos")

        if process.poll() is not None:
            break

        time.sleep(1)

    if os.path.exists(saida_arquivo):
        with open(saida_arquivo, "r", encoding="utf-8") as f:
            saida_final = f.read()
        saida_final_limpa = limpar_ansi(saida_final)
        output_box.markdown(f"```text\n{saida_final_limpa}\n```")

        ai_analysis(saida_final_limpa)

    if os.path.exists(saida_arquivo):
        os.remove(saida_arquivo)
    if os.path.exists(script_temp):
        os.remove(script_temp)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3) Estado persistente
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3) Estado persistente (adaptado para duas IAs)
if "chat" not in st.session_state:
    # Cria o comitÃª com duas IAs
    
    # IA 1 - VisionÃ¡rio Criativo
    contexto_criativo = (
        "VocÃª Ã© um assistente de programaÃ§Ã£o com perfil VisionÃ¡rio Criativo.\n"
        "Sua missÃ£o Ã© gerar ideias novas, propor soluÃ§Ãµes inovadoras e ousadas.\n"
        "VocÃª deve incentivar abordagens fora do convencional.\n"
        "Aceita certo grau de risco e imperfeiÃ§Ã£o, priorizando inovaÃ§Ã£o sobre conservadorismo.\n"
        "Seja livre, inventivo e sempre proponha algo novo, mesmo que seja improvÃ¡vel."
    )
    ia_generator(contexto_criativo)

    # IA 2 - Analista CrÃ­tico
    contexto_critico = (
        "VocÃª Ã© um assistente de programaÃ§Ã£o com perfil Analista CrÃ­tico.\n"
        "Sua missÃ£o Ã© revisar cuidadosamente as soluÃ§Ãµes, buscando falhas, incoerÃªncias e riscos.\n"
        "VocÃª deve ser rigoroso, tÃ©cnico e meticuloso.\n"
        "Questione tudo que parecer impraticÃ¡vel ou mal fundamentado.\n"
        "Seja frio, lÃ³gico e priorize a solidez sobre a ousadia."
    )
    ia_generator(contexto_critico)

# Define o chat principal como a primeira IA (VisionÃ¡rio Criativo)
chat = comite_ias[0]
messages = st.session_state.setdefault("messages", [])
total_tokens = st.session_state.setdefault("total_tokens", 0)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4) CabeÃ§alho
st.title("ğŸ—¨ï¸ Gemini Chatbot")
st.caption(f"Modelo: **{MODEL_ID}** ï½œ Projeto: **{PROJECT_ID}**")
st.markdown("---")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5) HistÃ³rico
for role, txt in messages:
    lbl = (
        "VocÃª" if role == "user" else
        "Gemini" if role == "bot" else
        "âš™ï¸ ExecuÃ§Ã£o" if role == "execucao" else
        "ğŸ” AnÃ¡lise" if role == "analise" else
        "ğŸ’­ Sistema"
    )
    st.chat_message(lbl).markdown(txt)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6) Caixa de entrada
prompt = st.chat_input("Digite sua pergunta...")

if prompt:
    st.chat_message("VocÃª").markdown(prompt)
    messages.append(("user", prompt))

    st.chat_message("ğŸ’­ Sistema").markdown("ğŸ’­ **IA pensando...**")
    messages.append(("system", "ğŸ’­ **IA pensando...**"))

    # Envia o prompt diretamente para o comitÃª
    ai_analysis(prompt)
