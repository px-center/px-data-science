import os, re, subprocess, time, uuid
import streamlit as st
import vertexai
from vertexai.preview.generative_models import GenerativeModel

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1) ConfiguraÃ§Ã£o inicial
st.set_page_config(page_title="Gemini Chat", page_icon="ğŸ¤–")

os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = ".formal-purpose-354320-af3d391f5234.json"
PROJECT_ID = "formal-purpose-354320"
LOCATION   = os.getenv("GOOGLE_CLOUD_REGION", "us-central1")
MODEL_ID   = "gemini-2.0-flash"

vertexai.init(project=PROJECT_ID, location=LOCATION)
model = GenerativeModel(MODEL_ID)

def limpar_ansi(texto):
    texto = re.sub(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])', '', texto)
    linhas = texto.splitlines()
    linhas_limpa = [linha for linha in linhas if linha.strip()]
    return "\n".join(linhas_limpa)

def estimar_tokens(texto):
    return len(texto) / 4  # AproximaÃ§Ã£o: 1 token â‰ˆ 4 caracteres

def executar_codigos(texto, profundidade=0, max_profundidade=50000):
    if profundidade >= max_profundidade:
        st.warning("ğŸ”„ Limite mÃ¡ximo de ciclos de execuÃ§Ã£o atingido!")
        return

    python_blocks = re.findall(r"```python\s+(.*?)```", texto, re.DOTALL | re.IGNORECASE)
    if not python_blocks:
        return

    codigo = "\n\n".join(python_blocks)
    script_temp = "temp_script.py"
    saida_arquivo = "saida_codigo.txt"

    with open(script_temp, "w", encoding="utf-8") as f:
        f.write(codigo)

    with open(saida_arquivo, "w", encoding="utf-8") as f_saida:
        process = subprocess.Popen(
            ["python", script_temp],
            stdout=f_saida,
            stderr=subprocess.STDOUT,
            text=True
        )

    output_box = st.empty()
    tempo_box = st.empty()
    cancelar_box = st.empty()

    tempo_inicio = time.time()
    cancelado = False

    messages.append(("execucao", "ğŸš€ **Executando cÃ³digo Python...**"))

    while True:
        if cancelar_box.button("âŒ Cancelar ExecuÃ§Ã£o", key=f"cancelar_execucao_{uuid.uuid4()}"):
            if process.poll() is None:
                process.terminate()
                cancelado = True

        if os.path.exists(saida_arquivo):
            with open(saida_arquivo, "r", encoding="utf-8") as f:
                content = f.read()
            content_limpo = limpar_ansi(content)
            output_box.markdown(f"```text\n{content_limpo}\n```")

        tempo_decorrido = time.time() - tempo_inicio
        tempo_box.markdown(f"â±ï¸ **Tempo de execuÃ§Ã£o:** {tempo_decorrido:.1f} segundos")

        if process.poll() is not None:
            break

        time.sleep(1)

    if os.path.exists(saida_arquivo):
        with open(saida_arquivo, "r", encoding="utf-8") as f:
            saida_final = f.read()
        saida_final_limpa = limpar_ansi(saida_final)
        output_box.markdown(f"```text\n{saida_final_limpa}\n```")

    analise_msg = "ğŸ” **Analisando resultados da execuÃ§Ã£o...**"
    st.chat_message("ğŸ’­ Sistema").markdown(analise_msg)
    messages.append(("system", analise_msg))

    nova_resposta = chat.send_message(saida_final_limpa).text
    st.chat_message("Gemini").markdown(nova_resposta)
    messages.append(("bot", nova_resposta))

    # Atualizar tokens
    tokens_saida_follow = estimar_tokens(nova_resposta)
    total_tokens = st.session_state.get("total_tokens", 0) + tokens_saida_follow
    st.session_state["total_tokens"] = total_tokens

    st.markdown(
        f"<div style='color: gray; font-size: small;'>"
        f"Rodada: {int(tokens_saida_follow)} "
        f"Total: {int(total_tokens)}"
        f"</div>",
        unsafe_allow_html=True
    )

    if os.path.exists(saida_arquivo):
        os.remove(saida_arquivo)
    if os.path.exists(script_temp):
        os.remove(script_temp)

    # Recursivamente processar nova resposta
    executar_codigos(nova_resposta, profundidade=profundidade+1, max_profundidade=max_profundidade)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3) Estado persistente
chat             = st.session_state.setdefault("chat", model.start_chat())
messages         = st.session_state.setdefault("messages", [])
total_tokens     = st.session_state.setdefault("total_tokens", 0)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3.1) InstruÃ§Ã£o inicial
if "instrucoes_enviadas" not in st.session_state:
    instrucao_inicial = (
        "VocÃª Ã© um assistente de programaÃ§Ã£o.\n"
        "Seu objetivo Ã© ajudar o usuÃ¡rio a criar, melhorar e executar cÃ³digos Python.\n"
        "Sempre que gerar cÃ³digos Python, escreva-os em blocos entre crases e identifique com 'python'.\n"
        "Se precisar fazer anÃ¡lises de resultados, gere instruÃ§Ãµes claras e simples.\n"
        "Responda apenas com informaÃ§Ãµes tÃ©cnicas, sem floreios ou enfeites desnecessÃ¡rios.\n"
        "Seja objetivo, tÃ©cnico e profissional."
    )
    resposta_instrucao = chat.send_message(instrucao_inicial).text
    messages.append(("system", instrucao_inicial))
    messages.append(("bot", resposta_instrucao))
    st.session_state["instrucoes_enviadas"] = True

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4) CabeÃ§alho
st.title("ğŸ—¨ï¸ Gemini Chatbot")
st.caption(f"Modelo: **{MODEL_ID}** ï½œ Projeto: **{PROJECT_ID}**")
st.markdown("---")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5) HistÃ³rico
for role, txt in messages:
    lbl = (
        "VocÃª" if role == "user" else
        "Gemini" if role == "bot" else
        "âš™ï¸ ExecuÃ§Ã£o" if role == "execucao" else
        "ğŸ” AnÃ¡lise" if role == "analise" else
        "ğŸ’­ Sistema"
    )
    st.chat_message(lbl).markdown(txt)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6) Caixa de entrada
prompt = st.chat_input("Digite sua pergunta...")

if prompt:
    st.chat_message("VocÃª").markdown(prompt)
    messages.append(("user", prompt))

    st.chat_message("ğŸ’­ Sistema").markdown("ğŸ’­ **IA pensando...**")
    messages.append(("system", "ğŸ’­ **IA pensando...**"))

    answer = chat.send_message(prompt).text
    st.chat_message("Gemini").markdown(answer)
    messages.append(("bot", answer))

    # Tokens
    tokens_entrada = estimar_tokens(prompt)
    tokens_saida = estimar_tokens(answer)
    tokens_rodada = tokens_entrada + tokens_saida
    total_tokens += tokens_rodada
    st.session_state["total_tokens"] = total_tokens

    st.markdown(
        f"<div style='color: gray; font-size: small;'>"
        f"Rodada: {int(tokens_rodada)} "
        f"Total: {int(total_tokens)}"
        f"</div>",
        unsafe_allow_html=True
    )

    # Agora sempre executa qualquer cÃ³digo, e reanalisa
    executar_codigos(answer)

#Adicionado pelo script de anÃ¡lise
