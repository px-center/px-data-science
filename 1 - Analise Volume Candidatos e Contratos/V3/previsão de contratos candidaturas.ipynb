{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao executar a query contract_canceled.sql: 403 POST https://bigquery.googleapis.com/bigquery/v2/projects/formal-purpose-354320/jobs?prettyPrint=false: Access Denied: Project formal-purpose-354320: User does not have bigquery.jobs.create permission in project formal-purpose-354320.\n",
      "\n",
      "Location: None\n",
      "Job ID: da2dd23f-1581-4658-9390-e14e52a6dcdc\n",
      "\n",
      "Erro geral: 403 POST https://bigquery.googleapis.com/bigquery/v2/projects/formal-purpose-354320/jobs?prettyPrint=false: Access Denied: Project formal-purpose-354320: User does not have bigquery.jobs.create permission in project formal-purpose-354320.\n",
      "\n",
      "Location: None\n",
      "Job ID: da2dd23f-1581-4658-9390-e14e52a6dcdc\n",
      "\n",
      "Conexão estabelecida com sucesso!\n"
     ]
    }
   ],
   "source": [
    "from lib import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')\n",
    "\n",
    "try:\n",
    "    dfs = load_data_bq()\n",
    "except Exception as e:\n",
    "    dfs = load_data_db()\n",
    "df_candidaturas = dfs[\"candidaturas\"].sort_values(\"created_at\")\n",
    "from datetime import timedelta\n",
    "df_candidaturas['created_at'] = df_candidaturas['created_at'].dt.strftime(\"%Y-%m-%d\")\n",
    "df_candidaturas[\"created_at\"] = pd.to_datetime(df_candidaturas[\"created_at\"])\n",
    "df_candidaturas[\"day_of_week\"] = df_candidaturas[\"created_at\"].dt.day_name()\n",
    "df_candidaturas[\"month\"] = df_candidaturas[\"created_at\"].dt.month_name()\n",
    "df_candidaturas[\"day_record_original\"] = (df_candidaturas[\"created_at\"] - df_candidaturas[\"created_at\"].min()).dt.days\n",
    "df_candidaturas[\"enum_month\"] = df_candidaturas[\"created_at\"].dt.month\n",
    "df_candidaturas[\"enum_day_of_week\"] = df_candidaturas[\"created_at\"].dt.day_of_week\n",
    "df_candidaturas = df_candidaturas.groupby([\"created_at\", \"driver_id\"]).apply(lambda x: x.sample(1)).reset_index(drop=True)\n",
    "df_candidaturas_original = df_candidaturas.copy()\n",
    "#1m23s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIM\n",
    "# FIM\n",
    "# FIM\n",
    "# FIM\n",
    "# FIM\n",
    "# FIM\n",
    "# FIM\n",
    "# FIM\n",
    "# FIM\n",
    "# FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_range = pd.date_range(start=df_candidaturas[\"created_at\"].min(), end=pd.to_datetime(\"2030-12-31\"), freq='D')\n",
    "date_range = pd.date_range(start=df_candidaturas[\"created_at\"].min(), end=pd.to_datetime(\"2030-03-14\"), freq='D')\n",
    "\n",
    "df_dates = pd.DataFrame(date_range, columns=[\"date\"])\n",
    "df_dates[\"date\"] = df_dates[\"date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "df_dates[\"date\"] = pd.to_datetime(df_dates[\"date\"])\n",
    "df_dates[\"month\"] = df_dates[\"date\"].dt.month_name()\n",
    "df_dates[\"day_of_week\"] = df_dates[\"date\"].dt.day_name()\n",
    "df_dates[\"day_record_prediction\"] = (df_dates[\"date\"] - df_candidaturas[\"created_at\"].min()).dt.days\n",
    "df_dates[\"enum_month_prediction\"] = df_dates[\"date\"].dt.month\n",
    "df_dates[\"enum_day_of_week_prediction\"] = df_dates[\"date\"].dt.day_of_week\n",
    "df_dates = df_dates.merge(df_candidaturas, left_on=[\"month\", \"day_of_week\"], right_on=[\"month\", \"day_of_week\"], how=\"left\")\n",
    "df_dates[\"count_by_day\"] = df_dates.groupby(\"date\")[\"date\"].transform(\"count\")\n",
    "\n",
    "#2.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preparar os dados\n",
    "df = df_candidaturas.copy()\n",
    "df = df.groupby([\"day_record_original\", \"enum_month\", \"enum_day_of_week\"])[\"created_at\"].count().reset_index()\n",
    "\n",
    "# Selecionar features (entradas) e alvo (target)\n",
    "features = [\"day_record_original\", \"enum_month\", \"enum_day_of_week\"]\n",
    "X = df[features].values\n",
    "y = df[\"created_at\"].values\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Modelo melhorado com Batch Normalization\n",
    "model = models.Sequential()\n",
    "\n",
    "# Camada de entrada com mais neurônios, BatchNormalization e ativação 'relu'\n",
    "model.add(layers.Dense(32, input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "# Camada oculta intermediária com BatchNormalization\n",
    "model.add(layers.Dense(16))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "# Camada oculta intermediária com BatchNormalization\n",
    "model.add(layers.Dense(8))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "# Camada de saída com ativação linear (padrão para regressão)\n",
    "model.add(layers.Dense(1, activation='relu'))\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Treinar o modelo e salvar o histórico\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=1000,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Avaliar o modelo no conjunto de teste\n",
    "loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(f\"Test Loss (MSE): {loss:.4f} - Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = model.predict(X_test)\n",
    "# print(\"Previsões:\", y_pred.flatten())\n",
    "\n",
    "# Plotar a curva de convergência do loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Curva de Convergência do Loss')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_candidaturas.copy()\n",
    "df = df.groupby([\"day_record_original\", \"enum_month\", \"enum_day_of_week\"])[\"created_at\"].count().reset_index()\n",
    "features = [\"day_record_original\", \"enum_month\", \"enum_day_of_week\"]\n",
    "X = df[features].values\n",
    "# scaler = MinMaxScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "y_pred = model.predict(X)\n",
    "df[\"pred\"] = y_pred\n",
    "plt.scatter(df[\"created_at\"], df[\"pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_candidaturas.copy()\n",
    "df = df.groupby([\"day_record_original\", \"enum_month\", \"enum_day_of_week\"])[\"created_at\"].count().reset_index()\n",
    "df[\"day_record_original\"]  = df[\"day_record_original\"]\n",
    "features = [\"day_record_original\", \"enum_month\", \"enum_day_of_week\"]\n",
    "X = df[features].values\n",
    "y_pred = model.predict(X)\n",
    "df[\"pred\"] = y_pred\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(df[\"day_record_original\"], df[\"created_at\"], color=\"blue\", label=\"Dados originais\")\n",
    "plt.scatter(df[\"day_record_original\"], df[\"pred\"], color=\"red\", label=\"Modelo ajustado\")\n",
    "plt.title(\"Ajuste do modelo aos dados reais\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_dates.copy()\n",
    "df = df.sample(50000)\n",
    "features = [\"day_record_prediction\", \"enum_month\", \"enum_day_of_week\"]\n",
    "X = df[features].values\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "df_dates = df.copy()\n",
    "df_dates[\"num_of_contracts_predict\"] = y_pred\n",
    "df_dates[\"weight\"] =  df_dates[\"count_by_day\"] / df_dates[\"count_by_day\"].mean() * df_dates[\"num_of_contracts_predict\"] \n",
    "df_dates[\"weight\"] = df_dates[\"weight\"] / df_dates.groupby(\"day_record_prediction\")[\"num_of_contracts_predict\"].transform(\"count\")\n",
    "#7,8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = df_candidaturas.copy()\n",
    "df = df.groupby(\"day_record_original\")[\"created_at\"].count().reset_index()\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.scatter(df[\"day_record_original\"], df[\"created_at\"], color=\"blue\", marker=\"o\")\n",
    "plt.xlabel(\"Day Record Prediction\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.title(\"Crossplot: Day Record Prediction vs Weight\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "\n",
    "df = df_dates.copy()\n",
    "df = df.groupby(\"day_record_prediction\")[\"weight\"].sum().reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(df[\"day_record_prediction\"], df[\"weight\"], color=\"red\", marker=\"o\")\n",
    "plt.xlabel(\"Day Record Prediction\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.title(\"Crossplot: Day Record Prediction vs Weight\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Carrega o DataFrame da região e remove duplicatas para garantir unicidade\n",
    "df_region = pd.read_csv(\"data/RELATORIO_DTB_BRASIL_DISTRITO.csv\", delimiter=\";\")\n",
    "df_region = df_region[[\"Nome_UF\", \"Nome_Município\", \"Nome_Mesorregião\"]].drop_duplicates()\n",
    "\n",
    "# Supondo que df_dates já exista, faça uma cópia para os resultados\n",
    "df_resultados = df_dates.copy()\n",
    "\n",
    "# Realiza o merge\n",
    "df_resultados = df_resultados.merge(\n",
    "    df_region, \n",
    "    how=\"left\", \n",
    "    left_on=[\"state\", \"city\"],\n",
    "    right_on=[\"Nome_UF\", \"Nome_Município\"]\n",
    ")\n",
    "\n",
    "# Remove as colunas desnecessárias e reorganiza as colunas\n",
    "df_resultados = df_resultados.drop([\"Nome_UF\", \"Nome_Município\"], axis=1)\n",
    "\n",
    "columns = [\n",
    "    'date', 'month', 'day_of_week', 'city', 'state', 'freights_type',\n",
    "    'contract_days', 'contract_values', 'vehicle', 'supply', 'cnh_category', 'Nome_Mesorregião', 'weight'\n",
    "]\n",
    "df_resultados = df_resultados[columns]\n",
    "\n",
    "df_resultados.columns = [\n",
    "    'date', 'month', 'day_of_week', 'city', 'state', 'freights_type',\n",
    "    'contract_days', 'contract_values', 'vehicle', 'supply', 'cnh_category', 'region', 'weight'\n",
    "]\n",
    "\n",
    "df_resultados = df_resultados.sort_values(\"date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_resultados.head())\n",
    "import json\n",
    "df = df_resultados\n",
    "exclude_cols = ['date', 'weight']\n",
    "json_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "df['data_json'] = df[json_cols].apply(lambda row: json.dumps(row.to_dict()), axis=1)\n",
    "df_resultados = df[['date', 'weight', 'data_json']]\n",
    "display(df_resultados.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultados.to_csv(\"data/simulacao_candidatos\")\n",
    "from lib_volume import *\n",
    "insert_into_bigquery(df_resultados, table_id = \"simulated_future_candidates\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".estudo_volumes_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
